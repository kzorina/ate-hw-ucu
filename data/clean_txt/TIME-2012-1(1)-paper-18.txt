2012 19th International Symposium on Temporal Representation and Reasoning  A Review on Temporal Reasoning using Support Vector Machines  Renata C. B. Madeo, Clodoaldo A. M. Lima, Sarajane M. Peres  School of Arts, Sciences and Humanities  University of SaĚo Paulo  SaĚo Paulo, Brazil  {renata.si, c.lima, sarajane}@usp.br  data in order to incorporate temporal dynamics into each  datapoint to be submitted to traditional SVM models; or  developing mathematical models considering time, and use  a traditional SVM to estimate some parameters for these  models. Otherwise, regarding approaches that deal with  temporal aspects in a internal way, we can adapt SVM model  to incorporate temporal reasoning, building more complex  strategies that are able to interpret and take advantage of  the temporal dependencies among the data.  In this paper we present a review on studies that incorporate temporal reasoning into SVM, considering mainly  approaches applied in researches executed in the last ďŹve  years. This paper is organized as follows: Section II and  Section III brieďŹy presents traditional SVM and some of its  variation; Section IV describes approaches that incorporate  temporal reasoning into SVM; ďŹnally, our ďŹnal considerations are delineated in Section V.  AbstractâRecently, Support Vector Machines have presented promissing results to various machine learning tasks,  such as classiďŹcation and regression. These good results have  motivated its application to several complex problems, including temporal information analysis. In this context, some studies  attempt to extract temporal features from data and submit  these features in a vector representation to traditional Support  Vector Machines. However, Support Vector Machines and its  traditional variations do not consider temporal dependency  among data. Thus, some approaches adapt Support Vector  Machines internal mechanism in order to integrate some  processing of temporal characteristics, attempting to make  them able to interpret the temporal information inherent on  data. This paper presents a review on studies covering this last  approach for dealing with temporal information: incorporating  temporal reasoning into Support Vector Machines and its  variations.  Keywords-Support Vector Machine; temporal reasoning; machine learning;  I. I NTRODUCTION  II. S UPPORT V ECTOR M ACHINES  In the last decade, there has been a great increase the use  of Support Vector Machines (SVM) in various applications.  The growing interest in this technique is justiďŹed by its  good performance, presented in different studies applied to  complex problems, including problems with temporal data.  Traditional SVM approaches aim at processing independent and identically distributed (iid) data, ignoring temporal  dependencies among instances. Therefore, for employing  traditional SVM to temporal problems, it is necessary to  extract temporal features, incorporate these features into a  vector representation, and process them just as they were iid  data. Although this naive approach has provided some good  results, it ignores important properties of the data.  We have adopted a taxonomy discussed in [1] in order to  organize different SVM approaches regarding temporal data  analysis. First, as aforementioned, problems related to temporal analysis can be treated without considering, directly,  temporal aspects of the data. Although this approach does  not take advantage of temporal dependencies within data, it  is still very common because of its simplicity. If we consider  the temporal aspects, time can be externally or internally  processed with respect to SVM model. In order to treat time  externally, we can build implict or explicit temporal data  representations, which consist of, respectively, preprocessing  SVM are based on performing a non-linear mapping on  input vectors from their original input space to a highdimensional feature space; and optimize an hyperplane capable of separating data in this high-dimensional feature space.  This section describes SVM formulation for classiďŹcation  and regression problems.  1530-1311/12 $26.00 ÂŠ 2012 IEEE  DOI 10.1109/TIME.2012.15  A. ClassiďŹcation Problem  Considering a training set with đ samples, deďŹned by  đ  {đđ , đŚđ }đ  đ=1 , with input đđ â â and output đŚđ â {â1, +1}.  SVM aims at ďŹnding an optimal hyperplane which separates  the datapoints in the feature space. Such hyperplane is given  by  đ (đĽ) = â¨đ â đ(đđ )âŠ + đ,  where đ is the optimal set of weights, đ(đđ ) represents a  nonlinear mapping on đđ , đ is the optimal bias, and â¨ââŠ is a  dot product.  In order to optimize separating hyperplane, SVM aims at  maximizing the functional margin, i.e., the distance between  the hyperplane and the closest datapoints from the hyperplane. As shown in [2], maximizing the margin corresponds  to minimizing the set of weights đ. The same author also  states that, in order to achieve a good generalization, it  114  An đźđ is assigned to each input vector. After training, all  non-zero đźđ are called Support Vectors (SV).  In the models obtained with Lagrangian method, the terms  đ(đĽđ ) and đ(đĽđ ) always appear multiplied. This fact allows  us to perform an implicit nonlinear mapping to a highdimensional feature space through kernels. This approach  is based on Cover Theorem, which states that a feature  space with non-linearly separable data can be mapped with  high probability into a input space where the data is linearly  separable, provided that the mapping is non-linear and the  feature space dimension is high enough [2]. Most common  kernel functions can be seen in Table I.  is necessary to minimize the Vapnik-Chervonenkis (VC)  dimension, which measures the capacity of the family of  functions realized by a learning machine. Thus, minimizing  đ also corresponds to minimizing VC dimension and ďŹnding  an optimal hyperplane provides a SVM with the smallest VC  dimension necessary to solve the classiďŹcation problem.  For problems which are nonlinearly separable in the feature space, we have to consider a soft margin optimisation,  which assumes slack variables đđ . Such variable denotes the  training error for the đth sample, đ = 1, â â â , đ , in order to  ďŹnd an hyperplane allowing some classiďŹcation errors that  are minimized into the optimisation problem. There are two  ways to implement this soft-margin optimisation using a 2norm or 1-norm [3]. Considering 2-norm soft margin, it is  possible to ďŹnd the optimal hyperplane by minimizing:  Table I  M OST COMMON KERNEL FUNCTIONS [2].  đ  đśâ 2  1  â¨đ â đâŠ +  đ ,  2  2 đ=1 đ  min đ(đ) =  where đś is a regularization factor and đđ = âŁđŚđ â đ (đĽđ )âŁ,  subject to  đŚđ (â¨đ â đ(đđ )âŠ + đ) âĽ 1 â đđ ,  đ  â  đźđ â  đ=1  B. Regression Problem  Also, SVM may be adapted for regression tasks. The  technique is called Support Vector Regression (SVR) and  considers slack variables đđ decomposed in đđ and đËđ , which  represents, respectively, errors above and below real output.  In the same way as in classiďŹcation problems, the formulation of the regression problem depends on the loss function.  The most common loss function is the đ-insensitive loss  function, which is formulated as  đźđ đŚ đ = 0  đ=1  {  for đ = 1, 2, â â â , đ.  đżđ =  Already considering 1-norm soft margin, it is possible to  optimize a hyperplane by minimizing  min đ(đ¤) =  đ  â  1  â¨đ â đâŠ + đś  đđ ,  2  đ=1  đŚđ (â¨đ â đ(đđ )âŠ + đ) âĽ 1 â đđ ,  min đ(đ) =  đ = 1, ..., đ  đ = 1, â â â , đ.  đ  â  đ=1  đźđ â  đ  1 â  đźđ đźđ đŚđ đŚđ â¨đ(đđ ) â đ(đđ )âŠ,  2 đ,đ=1  đ (  )  â  1  â¨đ â đâŠ + đś  đđ + đËđ ,  2  đ=1  đđ , đËđ âĽ 0.  However, there are other loss functions, such as quadratic  loss function and Huber loss function [4]. The quadratic loss  function is deďŹned as  đźđ đŚ đ = 0  đżđđ˘đđ = (đ (đđ ) â đŚđ )2 ,  đ=1  đś âĽ đźđ âĽ 0,  (1)  đŚđ â â¨đ â đđ âŠ â đ â¤ đ + đđ  â¨đ â đđ âŠ + đ â¤ đ + đËđ  subject to  đ  â  if âŁđŚđ â đ (đđ )âŁ â¤ đ  if âŁđŚđ â đ (đđ )âŁ > đ,  subject to the restrictions  From this optimization problem, we can apply the Lagrangian method obtaining  max â1 (đś) =  0,  đŚđ â đ (đđ ) â đ,  where đ is a parameter deďŹned by the user which states  the maximum deviation that should be accepted by the  algorithm, that is, errors below đ are not considered errors.  In this case, the problem is formulated as minimizing  subject to  đđ âĽ 0,  (â¨đđ â đđ âŠ + 1)đ  (  )  âĽđđ âđđ âĽ2  exp  2  2đż  (  )  tanh đż(â¨đđ â đđ âŠ) + đ  đ = 1, ..., đ.  đ  1 â  1  đźđ đźđ đŚđ đŚđ â¨đ(đđ ) â đ(đđ )âŠ + đżđđ ,  2 đ,đ=1  đś  đźđ âĽ 0  Polynomial  Two Layer Perceptron  where đżđđ = 1 if đ = đ and đżđđ = 0 otherwise, subject to  đ  â  Function  Radial Basis Function  From this optimization problem, we can apply the Lagrangian method obtaining  max â1 (đś) =  Kernel  đ = 1, 2, â â â , đ.  which leads us to the problem formulation  115  min đ(đ) =  đ  đś â ( 2 Ë2 )  1  â¨đ â đâŠ +  đ + đđ ,  2  2 đ=1 đ  IV. A NALYSED M ETHODS  (2)  This section presents a systematic organization of the  papers selected by our review, covering the topic temporal reasoning using SVM. The section is organized as  follows: Section IV-A presents Recurrent LS-SVM; Section IV-B presents Support Vector Echo-State Machines;  ProďŹle-Dependent Support Vector Machines are described in  Section IV-C; and Section IV-D and Section IV-E presents  modiďŹed kernels for dealing with temporal data, considering  respectively recurrent kernels and sequential kernel.  subject to  đŚđ â â¨đ â đđ âŠ â đ â¤ đđ  â¨đ â đđ âŠ + đ â¤ đËđ  đđ , đËđ âĽ 0.  The Huber loss function is deďŹned as  đżđťđ˘đđđ =  â§  â¨  1  (đ (đđ )  2  â đŚđ ) 2 ,  âŠ đâŁđ (đđ â đŚđ )âŁ â  đ2  ,  2  if âŁđ (đđ â đŚđ )âŁ > đ  A. Recurrent Least-Squares Support Vector Machines  if âŁđ (đđ â đŚđ )âŁ â¤ đ.  Recurrent LS-SVM (RLS-SVM) is mostly applied to time  series forecasting [7]. The idea is to consider as data a series  of input data đ˘đ and a series of output data đŚđ and an  autonomous recurrent model such as  leading to a problem formulation similar to (2), as described  in [4].  III. L EAST-S QUARES S UPPORT V ECTOR M ACHINES  đŚËđ = đ (Ë  đŚđâ1 , đŚËđâ2 , ..., đŚËđâđ ),  In [5], [6], a least squares type of SVM was introduced by  changing the problem formulation so as to yield a linear set  of equations in the dual space. This is done by taking a least  squares cost function, with equality instead of inequality  constraints. This modiďŹcation enable the classiďŹer to be  solved through a set of linear equations instead of quadratic  programming.  In LS-SVM, the classiďŹcation problem is formulated as  min đĽ2 (đ, đ, đ) =  where đŚËđ denotes an estimated output at the instant đ, đ is  a nonlinear mapping, and the value đ deďŹnes model order.  Thus, we have that the sequence of previously estimated  outputs are the input for the forecasting model.  Then, from SVM theory, we can consider  [Ë  đŚđâ1 , đŚËđâ2 , â â â , đŚËđâđ ] as input1 for the model. Also,  the model is formulated in terms of error variables, i.e,  RLS-SVM training depends on the error between estimated  output and actual output [7]. Thus, the training is deďŹned  as the following optimization problem:  đ  đś â 2  1  â¨đ â đâŠ +  đ ,  2  2 đ=1 đ  subject to the equality constraints  đŚđ [â¨đ â đ(đđ )âŠ + đ] = 1 â đđ ,  minđĽ (đ, đ, đ) =  đ = 1, â â â , đ.  Applying Lagrangian method, we obtain  đ+đ  đś â 2  1  â¨đ â đâŠ +  đ ,  2  2 đ=đ+1 đ  (3)  subject to:  max â2 (đ, đ, đ; đź) = đ˝(đ, đ, đ)  â  đ  â  đŚđ â đđâ1 = â¨đ â đ(đđâ1âŁđâđ )âŠ + đ,  đźđ {đŚđ [â¨đ â đ(đđ )âŠ + đ] â 1 + đđ },  where đ is the lenght of the time series, đđ = đŚđ â đŚËđ repđŚđâ1 , đŚËđâ2 , â â â , đŚËđâđ ] â  resents the error, and đđâ1âŁđâđ = [Ë  [đđâ1 , đđâ2 , â â â , đđâđ ]. Then, applying Lagrangian method,  we obtain  đ=1  with the following optimality conditions:  đ  â  ââ  đźđ đŚđ đđđ = 0,  =0âđâ  âđ¤  đ=1  max â(đ, đ, đ; đś) = đ˝(đ, đ, đ)  đ  â  [  ]  +  đźđâđ Ă đŚđ â đđâ1 â â¨đ â đ(đđâ1âŁđâđ )âŠ â đ ,  đ  â  ââ  đźđ đŚđ = 0,  =0â  âđ  đ=1  ââ  = 0 â đźđ = đśđđ đ = 1, â â â , đ,  âđđ  ââ  = 0 â đŚđ [â¨đ â đ(đđ )âŠ + đ] â 1 + đđ = 0,  âđźđ  đ = đ + 1, â â â , đ + đ,  đ=1  subject to (4), (5), (6), and (7).  As đ can be deďŹned in terms of đś, it is possible to  discard the constraint (4), however, even discarding it, it is  still computionally expensive to ďŹnd a solution considering  all the other constraints, specially (6). Thus, it is possible  to consider the case when đś â â, which corresponds to  ignoring the ďŹrst term in (3), i.e., aiming only at minimizing  errors, subject to (5) and (7).  đ = 1, â â â , đ.  This set of equations can be written as the solution of  a set of linear equations [5]. Thus, it is easier to calculate  a solution for LS-SVM than for SVM. However, LS-SVM  has a disadvantage: the number of SV is proportional to the  errors at the datapoints, losing sparsity provided by SVM.  Also, as in SVM, LS-SVM can be adapted for regression  tasks [7].  1 In our notation, [đŚ  đâ1 , đŚđâ2 , â â â , đŚđâđ ] is, therefore, equivalent to  đđ in the SVM model input.  116  â§  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  â¨  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  âŠ  đ+đ  â  ââ  =đâ  đźđâđ đ(đđâ1âŁđâđ ) = 0  âđ¤  đ=đ+1  (4)  đ+đ  â  ââ  =  đźđâđ = 0  âđ  đ=đ+1  (5)  đ  â  ]  ââ  â [  â¨đ â đ(đđâ1âŁđâđ )âŠ = 0,  = đśđđ â đźđâđ â  đźđâđ+đ  âđđ  âđđâđ  đ=1  ââ  = đŚđ â đđ â â¨đ â đ(đđâ1âŁđâđ )âŠ â đ = 0,  âđźđâđ  Some particularities of this approach must be highlighted:  ďŹrst, it works only for unidimensional time series, since it  uses previous estimated outputs to provide an estimation  for the next; second, this approach considers đś â â,  which means it does not control VC-dimension and does  not aim at maximizing the margin. In order to overcome  these problems, [8] proposes a multidimensional RLS-SVM  and [9] proposes some methods for controlling the parameter  đś. Also, in [10], RLS-SVM is used with a mixed kernel in  order to obtain improve accuracy.  1) Regularized Recurrent Least-Squares Support Vector  Machines: As aforementioned, the RLS-SVM ignores the  regularization terms đś, due to the computational cost of  calculating the derivative in the constraint (6).  However, [9] argues that this simpliďŹcation is not really  necessary. There are two other options: the former option is  called regularized partially RLS-SVM by [8]. It consists on  disconsidering the summation term in (6) and considering  the optimization problem regarding to (3) as a whole. The  other option consists in considering all effects of parameter  đś and the equations (3) and (6) by deriving the summation  term in (6) into a set of nonlinear equation which must be  solved nummerically [9].  2) Multidimensional Recurrent Least-Squares Support  Vector Machines: The RLS-SVM was developed to deal  with unidimensional data, which is useful for some applications, such as forecasting unidimensional time series [7].  However, most applications may use multidimensional data.  A multidimensional RLS-SVM can be deďŹned by dividing  a multidimensional regression problem into a series of  unidimensional problems [8].  Consider a time series given by {đ1 , đ2 , â â â , đđ }, đĽđ â  âđ , and a problem with input đĽđâđ and output đŚđ â âđ  corresponding to an đ-th element of the time series, where đ  is a time delay. In order to tackle this problem, the authors  in [8] propose to convert a đ-dimensional problem to đ  unidimensional problems considering a weight matrix with  đ weight vectors; and đ bias. Incorporating this approach  in (3), the problem can be formulated as  min â(đđ , đđ , đđ,đ ) =  đ = đ + 1, â â â , đ  đ = đ + 1, â â â , đ  (6)  (7)  subject to  â§  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  â¨  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  ďŁ´  âŠ  (1)  â đđ,1 = â¨đ1 â đ1 (đŚđâ1 â đđâ1 )âŠ + đ1 ,  (2)  đŚđ  â đđ,2 = â¨đ2 â đ2 (đŚđâ1 â đđâ1 )âŠ + đ2 ,  đŚđ  (8)  .  .  .  (đ)  đŚđ  â đđ,đ = â¨đđ â đđ (đŚđâ1 â đđâ1 )âŠ + đđ ,  where đ = đ + 1, đ = 2, . . . , đ + đ â 1 and đđ = đŚđ â đŚËđ .  This technique may also be applied to unidimensional  time series, as long as this time series is reconstructed  through Phase Space Reconstruction generating a multidimensional time series.  3) Recurrent Least-Squares Support Vector Machines  with Mixed Kernel: RLS-SVM with a mixed kernel consists using a RLS-SVM with a kernel which combines  Radial-Basis Function (RBF) and Polynomial Kernel (Poly),  weighted by the parameter đ, as  (  )  (  )  (  )  đžđđđĽ đđ , đđ = đđžđđľđš đđ , đđ + (1 â đ)đžđ đđđŚ đđ , đđ .  In this context, the objective is to combine the strenghts  of each approach: RBF is a local function, having stronger  learning ability and weaker generalization ability; polynomial kernel is a global function, having stronger generalization ability and weaker learning ability [10]. Also, in the  same paper is proposed the use of a Genetic Algorithm in  order to optimize the parameters of the mixed kernel, i.e.,  the parameters of RBF, polynomial kernel and đ.  In order to evaluate the approach, the authors applied it to  forecast a time series based on Rossler function. For these  data, the use of RLS-SVM with the mixed kernel presented  better results than RBF kernel.  B. Support Vector Echo-State Machines  Another strategy for enabling SVM to perform a temporal analysis is called Support Vector Echo-State Machines  (SVESM) and uses Reservoir Computing [4]. Reservoir  Computing (RC) is a research ďŹeld which comprises techniques and methods for building Recurrent Neural Networks  (RNN) using a âreservoirâ. A reservoir is a large neural  network, with randomly and sparsely connected neurons.  Each neuron have an internal representation called states,  đ  đ  đ  â  â  1â  1  â¨đđ â đđ âŠ + đś  đ2 ,  2 đ=1  2 đ=đ+1 đ=1 đ,đ  117  which keep some information about the previous states,  meaning that the reservoir has memory. Within the reservoir,  input weights and internal weights are deďŹned randomly and  are not trained.  The SVESM is based on Echo-State Networks (ESN) that  is a RNN which uses a reservoir to perform an accurate  single-step prediction and then iterates in order to obtain  multiple-step predictions [4]. The idea is that only the  output weights must be trained, i.e., the reservoir performs  a nonlinear mapping and a output weights are trained to  perform a regression from reservoir state to the desired  output. Its equation can be written as  (or conďŹdence) of certain sample. It is possible to use this  penalization factor to implement an exponential memory  decay based on the conďŹdence of past samples [13], as  đđ = đđĄđ âđĄđ ,  đ â [0, 1],  where đđ is a weighting factor of parameter đś for the sample  at time đĄđ , đĄđ is the time for the đ-th sample and đĄđ is the  current time.  PD-SVM is also used for classiďŹcation problems [13]. In  this case, a weighting factor may be incorporated, depending  on the problem domain, in order to improve accuracy and  reduce false detections.  In [13], the classiďŹcation problem consists in classifying  the level of Cyclosporine A in the blood of a pacient.  There are three classes: below 150ng/mL, between 150 and  400ng/mL, and above 400ng/mL. As this problem contains  temporal aspects, the weighting factor incorporates memory  decay, aiming at favoring last samples, and also increases  the penalization factor đś near the detection border in order  to reduce false detections, such suggested in  )  (  đĽ(đ + 1) = tansig đžđ â đđ + đžđđ â đ˘đ + đđ+1  đŚ(đ + 1) = â¨đ â đđ âŠ,  where đĄđđđ đđ denotes the hyperbolic tangent function, đđ  denotes the state variables in the reservoir, đ˘đ and đŚđ are the  input and output of the ESN, and đđ+1 is an optional noise  vector. đžđ , đžđđ and đ are the internal connections, input  and output weights, respectively.  As aforementioned, the reservoir provides a nonlinear  mapping from input space to a state space. The idea of  SVESM is using the âreservoir trickâ instead of the âkernel  trickâ to perform the nonlinear mapping [4]. Thus, input data  is preprocessed by the reservoir and the reservoir states are  used as input to a SVR with linear kernel, which performs  the regression in order to obtain the desired output.  This study also presents three types of loss function  which can be used in SVESM: quadratic loss function, đsensitive loss function, and Huber loss function, as described  in Section II-B.  Finally, the study presents some simulations in order  to evaluate SVESM performance. The method outperforms  classical methods â such as ESN, Multilayer Perceptron  [11], SVM and Self Organizing Maps [12]; except in the prediction of the benchmark Mackey-Glass time series without  noise and outliers, in which case ESN performs much better  than any other method. However, considering Mackey-Glass  time series with noise and outliers, SVESM shows more  robustness and outperforms ESN.  1  [đ2 đśđĄ + đś1 đđĄâ1 + đ0 ]  150  1  [đ2 đśđĄ + đś1 đđĄâ1 + đ0 ],  =  400  đś150,đĄ+1 =  đś400,đĄ+1  (9)  where đđ represent additional penalization factor, which can  be ďŹxed a priori or computed in an adaptive way [13].  D. Recursive kernels  Recursive kernels operate on two discrete time series  {đ(1), đ(2), â â â , đ(đ)} and {đ(1), đ(2), â â â , đ(đ)}2 [14].  These kernels are associated to an inďŹnite recurrent network,  that is, a recurrent network with a hidden layer modeled as  a continuous function3 . A recursive kernel đ at a time đĄ is  given by  đđĄ (đĽ, đŚ) =  â¨ÎŚ(đĽ(đĄ), ÎŚ(đĽ(đĄ â 1), ÎŚ(â â â ))) â ÎŚ(đŚ(đĄ), ÎŚ(đŚ(đĄ â 1), ÎŚ(â â â )))âŠ.  where ÎŚ(â, â) is a nonlinear mapping, đĽ(đĄ) is the current  input, and ÎŚ(đĽ(đĄ â 1), ÎŚ(â â â )) corrresponds to the inďŹnitedimension state vector, i.e., a nonlinear mapping applied  recursively to all past elements of the time series.  In [14], some examples of recursive kernels are presented.  These examples consider two parameters: đ for scaling the  inďŹnite-dimension state vector, and đđ for scaling the current  input. The stability of these kernels usually depends on the  choice of these parameters, as described in [14]. The list of  kernels includes:  C. ProďŹle-Dependent Support Vector Machines  ProďŹle-Dependent SVM (PD-SVM) method arises from  a common practice for classiďŹcation problems with unbalanced data [13]. In this kind of problem, it is possible to  deďŹne different penalization factors for different classes,  aiming at preventing false positives by favoring classes with  less samples. The same principle may be applied to time  series, since more recent time seriesâ samples may contain  more relevant information than older samples.  In PD-SVM, the overall penalization factor đś from SVM  is adjusted by a time-dependent weighting factor based on a  conďŹdent function, i.e., a fuction that measures the relevance  2 In this section, đĽ and đŚ are used to designated two time series that are  submitted to the kernel function. As this section covers only kernel function  and do not cover SVM formulation, it is not necessary to refer to SVM  output. Thus, đŚ does not corresponds to an output; it corresponds to one  of the inputs submitted to the kernel function.  3 For further details about inďŹnite recurrent network and its relation to  recursive kernels, see [14].  118  â  Recursive linear kernel:  đđĄ (đĽ, đŚ) = đđ2  â  â  â  E. Sequence kernels  In this review, we found also found another type of  modiďŹed kernel for dealing with temporal aspects of data  called sequence kernels, which are capable of dealing with  sequences of vectors. One of these sequence kernels is  proposed by [15] and used by [16]. This sequence kernel  is based on Dynamic Time Warping (DTW) and it is called  polynomial DTW kernel.  DTW is a distance measure used for calculating distance  between two sequences of vectors. According to [15], symmetric DTW consists in considering a local distance between  two vectors and a global distance, which is calculated using  local distances and, indeed measures the distance between  the two sequences of vectors.  Also, it is possible that these two sequences of vector do  not have the same length. In this case, it is necessary to  perform an alignment between the sequences. Considering  đ´ and đľ as sequences composed of vectors đ(đ) and đ(đ),  this alignment may be denoted by đđ (đ) and đđ (đ) for  đ = {1, â â â , đź}, where đź is the length of the alignment,  and consists in linking each vector in đ(đ) to a vector in  đ(đ). A DTW alignment distance is given by summiting  local distances between the vectors under analysis:  â¨đ 2đ đĽ(đĄ â đ) â đŚ(đĄ â đ)âŠ.  đ=0  Recursive polynomial kernel:  [  ]đ  đđĄ (đĽ, đŚ) = â¨đđ2 đĽ(đĄ) â đŚ(đĄ)âŠ + đ 2 đđĄâ1 (đĽ, đŚ) .  â  Recursive gaussian (also known as Radial Basis Function â RBF) kernel:  đđĄ (đĽ, đŚ) =  exp  â  (  )  (  )  âĽđĽ(đĄ) â đŚ(đĄ)âĽ2  đđĄâ1 (đĽ, đŚ) â 1  â  exp  .  2  2  2đđ  đ  Recursive arcsine kernel:  đđĄ (đĽ, đŚ) =  2  arcsin  đ  ( (  ))  2 đđ2 â¨đĽ(đĄ) â đŚ(đĄ)âŠ + đ 2 đđĄâ1 (đĽ, đŚ) + đđ2  â  ,  đđĄ (đĽ)đđĄ (đŚ)  with  )  (  đđĄ (đĽ) = 1 + 2 đđ2 âĽđĽ(đĄ)âĽ2 + đ 2 đđĄâ1 (đĽ, đĽ) + đđ2 .  It is important to note that SVM with recursive kernel  works just as regular SVM. The differences are: the kernel  function considering recursion; and the input data which  composes the training set must include đ elements of the  past of the time series, where đ corresponds to the depth  recursion. Some experiments in [14] show the application of  recursive kernels to regression and classiďŹcation problems.  As regression problem, a benchmark in time series processing called NARMA (nonlinear auto regressive moving  average) was considered. For comparing results, a windowed  approach using SVM with RBF kernel, considering 27  frames as the size of the window, and an ESN were applied  to the problem. Both recursive kernels applied (RBF and  arcsine with recursion depth of 50 frames) provided better  results than classical approaches when all parameters were  correctly set, with recursive RBF kernel performing slightly  better than the recursive arcsine kernel.  As classiďŹcation problem, the authors in [14] considered  a phoneme recognition problem, aiming at recognizing 39  symbols representing phonemes. Each frame of speech was  represented by a 39-dimensional feature vector obtained  using Mel frequency cepstral coefďŹcients analysis. In this  case, recursive RBF and recursive arcsine kernel were tested  against a windowed approach using SVM with RBF kernel  considering a window of 9 frames. The recursion depth of  the recursive kernels varied according to the experiment,  from 5 to 15 frames. The best results in [14] were obtained  using the recursive RBF kernel on subsampled data by a  factor of 5, i.e., the time series was âspeeded upâ by a  factor of 5, which corresponds to slowing down the effective  timescale of the classiďŹer [14].  đˇđđđđđ (đ´, đľ) =  đź  1â  đ(đđđ (đ) , đđđ (đ) ).  đź đ=1  Then, global distance is obtained by calculating the global  distance to each possible alignment and considering  đˇ(đ´, đľ) = min đˇđđđđđ (đ´, đľ).  (10)  In order to build a kernel, DTW global distance has to be  converted to a dot product. For this conversion, a technique  called Spherical Normalization is used. It consists on projecting the sampled vectors đ and đ on a unit hypersphere,  as described in [15], through4  đ  Ë= â  1  (đ2  +  đź2 )  [ ]  đ  .  đź  (11)  By deďŹnition, the smallest curve between the two projected vectors đ  Ë and Ëđ in a hypersphere is given by the angle  Ë â Ëđ = cos đđË,Ëđ ,  đđË,Ëđ between these points [15]. Considering đ  local DTW distance may be deďŹned as the dot product:  đđ (Ë  đ, Ëđ) = arccos(Ë  đ â Ëđ).  (12)  Since it is possible to calculate DTW considering sequences mapped into a hypersphere, it is possible to perform  DTW to get the global distance using (10), using (12) as  local distance. Then, a linear DTW kernel can be deďŹned  through the equation which reconverts DTW global distance  to a dot product:  Ë đľ)  Ë = cosđˇđ (đ´  Ë â đľ).  Ë  đžđđđđđđđˇđ đ (đ´,  4 Equation  119  11 is also applied to vector đ.  From this kernel, it is also possible to deďŹne the polynomial DTW kernel as:  â  Ë đľ)  Ë = cosđ đˇđ (đ´  Ë â đľ).  Ë  đžđđđđŚđˇđ đ (đ´,  â  Both studies [15] and [16] presents experiments showing  that polynomial DTW kernel can promote improvements on  accuracy for speech recognition tasks. The study [15] shows  that polynomial DTW kernel performs better than traditional  DTW and Hidden Markov Models (HMM) for recognizing  speech collecting from both normal and dysarthric speakers.  For normal speakers, SVM with polynomial DTW kernel  performs better when there is a small number of training  samples per word; for more than 5 training samples per  word, SVMđˇđ đ performs as well as HMM and better then  DTW. For dysarthric speakers, SVMđˇđ đ performs better  than HMM and DTW for any number of training samples per  word. In [16], SVM with polynomial DTW kernel performs  better than Multilayer Perceptron neural network and Elman  network for recognizing speech from dysarthric speakers.  However, its performance is comparable to SVM with RBF  kernel: one method outperforms the other depending on  speaker intelligibility.  Also, in [15], three other examples of sequence kernels are  cited: the Fisher kernel [17], the Generalized Discriminant  kernel [18] and the pair HMM kernel [19]. The Fisher  kernel incorporates a generative model into a discriminant  classiďŹer, by generating a kernel using the Fisher Score  on the choosen generative model. The Generalized Linear  Discriminant Sequence (GLDS) kernel is a sequence kernel  based on the Generalized linear Discriminant Scoring and  on a classiďŹer. Finally, the pair HMM kernel is based on  converting the matching function of pair HMM into a dot  product for using it as a kernel.  â  â  â  â  Multidimensional RLS-SVM [8]: converts a multidimensional problem into đ unidimensional problems,  allowing multidimensional time series forecasting;  RLS-SVM with Mixed Kernel [10]: combines kernels  with different characteristics, in order to provide more  efďŹcient non-linear mappings to RLS-SVM;  SVESM [4]: uses a reservoir to treat the temporal  information, performing an explicit non-linear mapping  instead of the implicit mapping performed by kernels;  PD-SVM [13]: assigns greater weights to more recent  data, in a context of sequential events, creating a new  viewpoint to analyze a dataset;  Recursive Kernels [14]: transforms the time series  under analysis in a set of series organized in a training  dataset, performing the analysis in a recursive way; it  can be applied to classiďŹcaton tasks using a windowed  approach to build the training dataset;  Sequential Kernels [15], [16], [17], [18], [19]: enables  the implict treatment of sequences of vectors, e.g.,  multidimensional time series.  Considering the objective of each strategy described here,  note that techniques related to RLS-SVM aim mainly at time  series forecasting.This phenomenon occurs because RLSSVM techniques work with output in the same domain as  inputs, since delayed outputs are used to compose the input  vector. The same does not occur with PD-SVM, SVESM and  modiďŹed kernels, since they incorporate temporal reasoning  in data processing, but do not use delayed outputs as input;  thus, these techniques can be used to provide outputs from  any domain.  Also, it is possible discuss about how the strategies modify  the SVM operation. PD-SVM only balances the penalization  factor đś using a time-dependent weighting factor, without  altering any other aspect of SVM training. In SVESM,  the SVM operation also does not change, since the main  difference is that the non-linear mapping, originally performed by kernels in tradition SVM approachs, is performed  by a reservoir. ModiďŹed kernels also do not modify SVM  operation, since only the kernel is affected. On the other  hand, RLS-SVM and its variations modify SVM formulation  and operation in order to process recurrence.  As we have shown in this review, incorporating temporal reasoning to SVM has provided interesting and useful  strategies for temporal data analysis. In this context, we  hope that this review supplies a basis on the subject, aiming  at supporting the development of others studies which can  provide further insights on the related area. Some initiatives  that will deserve our attention in our future efforts of  reaseach are:  V. F INAL C ONSIDERATIONS  This paper presented a review of research which combines  SVM and some kind of temporal reasoning to provide  strategies for data analysis that join the advantages of SVM  with the ability to explore temporal dependencies among  data. Although most strategies have been applied to timeseries forecasting [7], [9], [8], [10], [13], [4], there are also  the possibility to apply it for temporal data classiďŹcation  and regression [13], [14], [15], [16], [17], [18], [19]. The  main contribuition of each strategy presented here can be  summarize as follows:  â RLS-SVM [7]: modiďŹes SVM in order to incorporate  recurrence, including the modiďŹcation of the equations  that deďŹne SVM operation to consider the error for  previous output in SVM training;  â Regularized RLS-SVM [9]: introduces a way to balance  errors minimization with margin maximization, which  also corresponds to controlling VC-dimension, dealing  with a negative aspect of RLS-SVM;  â  120  a practical study around the approach discussed here,  applying them on benchmarks classiďŹcation, regression  or prediction problems, in order to support further  analysis about the advantages and limitations of each  approach;  an speciďŹc study about the feasibility of the approaches  discussed here in a gesture analysis problem (area of  interest to the present paperâs authors).  From these future works, the authors aim to achieve  enough expertise to propose some directions about when  each approach is more useful or feasible, and, ďŹnally, to  contribute with the improvement of the relation between  SVM, or even more general Machine Learning approaches,  with temporal data processing.  [9] H. Qu, Y. Oussar, G. Dreyfus, and W. Xu, âRegularized  recurrent least squares support vector machines,â in International Joint Conference on Bioinformatics, Systems Biology  and Intelligent Computing. IEEE Computer Society, aug.  2009, pp. 508â511.  â  [10] J. Xie, âTime series prediction based on recurrent ls-svm with  mixed kernel,â in Asia-PaciďŹc Conference on Information  Processing, vol. 1, jul. 2009, pp. 113 â116.  [11] F. Rosenblatt, Principles of Neurodynamics: Perceptrons and  the Theory of Brain Mechanisms. Spartan, 1962.  Acknowledgement  The authors thank SaĚo Paulo Research Foundation  (FAPESP), Brazil, for its ďŹnancial support through process  number 2011/04608-8. Also, the authors thank PhD Alexandre Ferreira Ramos, for the interesting insights about some  mathematical derivations used during the writing of this  paper.  [12] T. Kohonen, âThe self-organizing map,â Proceedings of the  IEEE, vol. 78, no. 9, pp. 1464 â1480, sep 1990.  [13] G. Camps-Valls, E. Soria-Olivas, J. Perez-Ruixo, F. PerezCruz, A. Artes-Rodriguez, and N. Jimenez-Torres, âTherapeutic drug monitoring of kidney transplant recipients using  proďŹled support vector machines,â IEEE Trans. on Systems,  Man, and Cybernetics, Part C: Applications and Reviews,  vol. 37, no. 3, pp. 359 â372, may 2007.  R EFERENCES  [14] M. Hermans and B. Schrauwen, âRecurrent kernel machines:  Computing with inďŹnite echo state networks,â Neural Computation, vol. 24, pp. 104â133, jan. 2012.  [1] J.-C. Chappelier and A. Grumbach, âTime in neural networks,â SIGART Bulletin, vol. 5, no. 3, pp. 3â11, jul. 1994.  [2] S. Haykin, Neural Networks: A Comprehensive Foundation,  2nd ed. Upper Saddle River, NJ: Prentice Hall, 1999.  [15] V. Wan and J. Carmichael, âPolynomial dynamic time warping kernel support vector machines for dysarthric speech  recognition with sparse training data,â in Annual Conference  of the International Speech Communication Association, sep.  2005, pp. 3321â3324.  [3] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-based Learning  Methods. Cambridge, UK: Cambridge University Press, mar.  2000.  [16] F. Rudzicz, âPhonological features in discriminative classiďŹcation of dysarthric speech,â in IEEE International Conference on Acoustics, Speech and Signal Processing, apr. 2009,  pp. 4605 â4608.  [4] Z. Shi and M. Han, âSupport vector echo-state machine  for chaotic time-series prediction,â IEEE Trans. on Neural  Networks, vol. 18, no. 2, pp. 359 â372, mar. 2007.  [5] J. Suykens and J. Vandewalle, âLeast squares support vector  machine classiďŹers,â Neural Processing Letters, vol. 9, pp.  293â300, jun. 1999.  [17] T. S. Jaakkola and D. Haussler, âExploiting generative models  in discriminative classiďŹers,â in Conference on Advances in  Neural Information Processing Systems. Cambridge, MA,  USA: MIT Press, nov. 1999, pp. 487â493.  [6] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor,  and J. Vandewalle, Least squares support vector machines.  World ScientiďŹc Pub., 2003.  [18] W. M. Campbell, âGeneralized linear discriminant sequence  kernels for speaker recognition,â in IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1,  may 2002, pp. Iâ161 âIâ164.  [7] J. A. K. Suykens and J. Vandewalle, âRecurrent least squares  support vector machines,â IEEE Trans. on Circuits and  Systems-I, vol. 47, pp. 1109â1114, jul. 2000.  [19] C. Watkins, âDynamic alignment kernels,â University of  London, Tech. Rep., 1999.  [8] J. Sun, C. Zheng, Y. Zhou, Y. Bai, and J. Luo, âNonlinear  noise reduction of chaotic time series based on multidimensional recurrent ls-svm,â Neurocomputing, vol. 71, pp. 3675â  3679, oct. 2008.  121 