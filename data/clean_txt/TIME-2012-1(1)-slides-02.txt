Reasoning about Plan Revision in Agent Programs  Natasha Alechina  University of Nottingham, UK  TIME 2012, Leicester 14 September 2012  Natasha Alechina  Reasoning about plan revision  TIME 2012  1 / 46  What this talk is about  verification (of agent programs with changing plans)  transition systems correspond to agent program execution  model-checking agent programs  joint work with Brian Logan, Mehdi Dastani and John-Jules Meyer  on a theorem-proving approach (using dynamic logic)  main extension: explicit operator for âhaving a planâ  Natasha Alechina  Reasoning about plan revision  TIME 2012  2 / 46  Transition systems  a  c  b  d  p  Natasha Alechina  Reasoning about plan revision  TIME 2012  3 / 46  Dynamic logic  ha; bip, hc; dip  a  c  b  d  p  Natasha Alechina  Reasoning about plan revision  TIME 2012  4 / 46  Having and executing a plan  Plan(a;b)  a  b  p  Natasha Alechina  Reasoning about plan revision  TIME 2012  5 / 46  What is an agent?  many definitions of âagentâ in the literature â key ideas include:  autonomy: an agent operates without the direct intervention of  humans or other agents  situatedness: an agent interacts with its environment (which may  contain other agents)  reactivity: an agent responds in a timely fashion to changes in its  environment  proactivity: an agent exhibits goal-directed behaviour  Natasha Alechina  Reasoning about plan revision  TIME 2012  6 / 46  What I will mean by an agent  a computational system whose behaviour can be usefully  characterised in terms of propositional attitudes such as beliefs  and goals  and which is programmed in an agent programming language  which makes explicit use of propositional attitudes  Natasha Alechina  Reasoning about plan revision  TIME 2012  7 / 46  What is an agent programming language?  Belief, Desire and Intentions (BDI) framework, (Bratman 1987)  BDI agent programming languages are designed to facilitate the  implementation of BDI agents:  programming constructs corresponding to beliefs, desires and  intentions  agent architecture or interpreter enforces relationships between  beliefs, desires and intentions and which causes the agent to  choose actions to achieve its goals based on its beliefs  Natasha Alechina  Reasoning about plan revision  TIME 2012  8 / 46  3APL  one of the first agent programming languages PRS (Georgeff and  Ingrand 1988), very rich. I will talk about a more modern and less  rich langauge, 3APL  3APL is a BDI agent programming language proposed in (Dastani  et al 2003)  I present a cut-down version of 3APL (mostly regarding the  language for beliefs, but also distinction between external and  internal actions, not considering messages etc.)  Natasha Alechina  Reasoning about plan revision  TIME 2012  9 / 46  3APL beliefs  the beliefs of a 3APL agent represent its information about its  environment and itself  beliefs are represented by a set of positive literals  the initial beliefs of an agent are specified by its program  e.g., the agent may initially believe that itâs in room1 and its battery  is charged:  Beliefs:  room1, battery  Natasha Alechina  Reasoning about plan revision  TIME 2012  10 / 46  3APL goals  the agentâs goals represent situations the agent wants to realise  (not necessarily all at once)  goals are represented by a set of arbitrary literals  the initial goals of an agent are specified by its program  e.g., the agent may initially want to achieve a situation in which  both room1 and room2 are clean  Goals:  clean1, clean2  Natasha Alechina  Reasoning about plan revision  TIME 2012  11 / 46  Declarative goals  the beliefs and goals of an agent are related to each other  if an agent believes p, then it will not pursue p as a goal  if an agent does not believe that p, it will not have â p as a goal  these relationships are enforced by the agent architecture  Natasha Alechina  Reasoning about plan revision  TIME 2012  12 / 46  3APL basic actions  basic actions specify the capabilities of the agent (what it can do  independent of any particular agent program)  2 types of basic actions:  belief test actions: test whether the agent has a given belief  belief update actions: âexternalâ actions which change the agentâs  beliefs  Natasha Alechina  Reasoning about plan revision  TIME 2012  13 / 46  Belief test actions  a belief test action Ď? tests whether a boolean belief expression Ď  is entailed by the agentâs beliefs, e.g. :  (room2 and -battery)?  tests whether the agent believes it is in room2 and its battery is  not charged  Natasha Alechina  Reasoning about plan revision  TIME 2012  14 / 46  Belief update actions  belief update actions change the beliefs (and goals) of the agent  a belief update action is specified in terms of its pre- and  postconditions (sets of literals), e.g. :  {room1} moveR {  }, {-room1, room2}  an action can be executed if one of its pre-conditions is entailed by  the agentâs current beliefs  executing the action updates the agentâs beliefs to make one of  the postconditions entailed by the agentâs beliefs (actions  non-deterministic)  Natasha Alechina  Reasoning about plan revision  TIME 2012  15 / 46  Belief entailment  a belief query (a belief test action or an action precondition) is  entailed by the agentâs belief base if  all positive literals in the query are contained in the agentâs belief  base, and  for every negative literal â p in the query, p is not in the belief base  i.e., we use entailment under the closed world assumption  goal entailment corresponds to a formula being classically  entailed by one of the goals in the goal base  Natasha Alechina  Reasoning about plan revision  TIME 2012  16 / 46  Belief update  executing a belief update action  adds all positive literals in the corresponding postcondition to the  belief base, and  for every negative literal â p in the postcondition, p is removed from  the agentâs belief base  goals which are achieved by the postcondition of an action are  dropped  for simplicity, we assume that the agentâs beliefs about its  environment are always correct and its actions in the environment  are always successful  Natasha Alechina  Reasoning about plan revision  TIME 2012  17 / 46  Abstract plans  unlike basic actions, abstract plans cannot be directly executed by  the agent.  abstract plans provide an abstraction mechanism (similar to  procedures in imperative programming) which are expanded into  basic actions using plan revision rules  if the first step of a plan Ď is an abstract plan ÎąĚ, execution of Ď  blocks.  Natasha Alechina  Reasoning about plan revision  TIME 2012  18 / 46  3APL plans  plans are sequences of basic actions and atomic plans composed  by plan composition operators:  sequence: âĎ1 ;Ď2 â (do Ď1 then Ď2 )  conditional choice: âif Ď then {Ď1 } else {Ď2 }â  conditional iteration: âwhile Ď do {Ď}â  e.g., the plan:  if room1 then {suck} else {moveL; suck}  causes the agent to clean room1 if itâs currently in room1,  otherwise it first moves (left) to room1 and then cleans it  Natasha Alechina  Reasoning about plan revision  TIME 2012  19 / 46  3APL PG rules  planning goal rules are used for plan selection based on the  agentâs current goals and beliefs  a planning goal rule Îş â Î˛ | Ď consists of three parts:  Îş: an (optional) goal query which specifies which goal(s) the plan  achieves  Î˛: a belief query which characterises the situation(s) in which it  could be a good idea to execute the plan  Ď: a plan  a PG rule can be applied if Îş is entailed by the agentâs goals and Î˛  is entailed by the agentâs beliefs  applying the rule adds Ď to the agentâs plans  Natasha Alechina  Reasoning about plan revision  TIME 2012  20 / 46  Example 3APL PG rules  clean2 <- battery |  if room2 then {suck} else {moveR; suck}  states that âif the agentâs goal is to clean room2 and its battery is  charged, then the specified plan may be used to clean the roomâ  an agent can generate a plan based only on its current beliefs  (reactive invocation), e.g., the rule:  <- -battery |  if room2 then {charge} else {moveR; charge}  states âif the battery is low, the specified plan may be used to  charge itâ  Natasha Alechina  Reasoning about plan revision  TIME 2012  21 / 46  Example 3APL PR rules  a plan revision rule pj = Ďj â Î˛j | Ď 0 j can be applied if Ďj is in the  plan base, Î˛j is entailed by the agentâs beliefs and Ďj is not  executable,  in other words the first action of Ďj is either a belief update or  belief test action which is not executable in the current belief state,  or an abstract plan  for example, if moveR fails, the agent may execute a slow but  reliable version of the action, slowR:  charge <- room1 |  {slowR; charge}  Natasha Alechina  Reasoning about plan revision  TIME 2012  22 / 46  Operational semantics  we define the operational semantics of 3APL in terms of a  transition system  states are agent configurations hĎ, Îł, Î i where Ď, Îł are sets of  literals representing the agentâs beliefs and goals, and Î  is a set of  plan entries representing the agentâs current active plans  (annotated by the goals which they were adopted to achieve)  each transition corresponds to a single step in the execution of the  agent  different execution strategies give rise to different semantics  for simplicity we focus on non-interleaved executionâi.e., the  agent executes a single plan to completion before choosing  another plan  Natasha Alechina  Reasoning about plan revision  TIME 2012  23 / 46  Formal entailment definitions  |=cwa (belief entailment for closed world assumption):  Ď |=cwa p iff p â Ď  Ď |=cwa âp iff p 6â Ď  Ď |=cwa Ď and Ď iff Ď |=cwa Ď and Ď |=cwa Ď  Ď |=cwa Ď or Ď iff Ď |=cwa Ď or Ď |=cwa Ď  Ď |=cwa {Ď1 , . . . , Ďn } iff â1 â¤ i â¤ n Ď |=cwa Ďi  |=g (goal entailment):  Îł |=g p iff p â Îł  Îł |=g âp iff âp â Îł  Îł |=g Ď or Ď iff Îł |=g Ď or Îł |=g Ď  Natasha Alechina  Reasoning about plan revision  TIME 2012  24 / 46  Belief update function  let a be a belief update action and Ď a belief base such that  Ď |=cwa precj (a)  intuitively, Ď |=cwa precj (a) if it contains all positive literals in  precj (a) and does not contain the negative ones  the result of executing belief update action a with respect to Ď  (assuming precj (a) holds and the action results in the postj,i  becoming true) is defined as:  Tj,i (a, Ď) = (Ď âŞ {p : p â postj,i (a)}) \ {p : â p â postj,i (a)}  intuitively, the result of the update satisfies (entails under |=cwa )  the corresponding postcondition postj,i (a)  Natasha Alechina  Reasoning about plan revision  TIME 2012  25 / 46  Transitions: belief test actions  belief test actions  Ď |=cwa Î˛  hĎ, Îł, {Î˛? ; Ď . Îş}i ââ hĎ, Îł, {Ď . Îş}i  Natasha Alechina  Reasoning about plan revision  TIME 2012  26 / 46  Transitions: belief update actions  belief update actions when the corresponding goal not achieved  yet:  Ď |=cwa preci (Îą) Ti,j (Îą, Ď) = Ď 0 Îł 0 = Îł \ {Ď | Ď 0 |=cwa Ď} Ď 0 6|=cwa Îş  hĎ, Îł, {Îą; Ď . Îş}i ââ hĎ 0 , Îł 0 , {Ď . Îş}i  belief update actions when the corresponding goal is achieved:  Ď |=cwa preci (Îą) Ti,j (Îą, Ď) = Ď 0 Îł 0 = Îł \ {Ď | Ď 0 |=cwa Ď} Ď 0 |=cwa Îş  hĎ, Îł, {Îą; Ď . Îş}i ââ hĎ 0 , Îł 0 , { }i  Natasha Alechina  Reasoning about plan revision  TIME 2012  27 / 46  Transitions: plans  conditional choice  Ď |=cwa Ď  hĎ, Îł, {(if Ď then Ď1 else Ď2 ); Ď . Îş}i ââ hĎ, Îł, {Ď1 ; Ď . Îş}i  Ď 6|=cwa Ď  hĎ, Îł, {(if Ď then Ď1 else Ď2 ); Ď . Îş}i ââ hĎ, Îł, {Ď2 ; Ď . Îş}i  conditional iteration  Ď |=cwa Ď  hĎ, Îł, {(while Ď do Ď1 ); Ď . Îş}i ââ hĎ, Îł, {Ď1 ; (while Ď do Ď1 ); Ď . Îş  Ď 6|=cwa Ď  hĎ, Îł, {(while Ď do Ď1 . Îş); Ď}i ââ hĎ, Îł, {Ď . Îş}i  Natasha Alechina  Reasoning about plan revision  TIME 2012  28 / 46  Transitions: PG rules  planning goal rules Îş â Î˛ | Ď  Îł |=g Îş Ďcwa |= Î˛  hĎ, Îł, {}i ââ hĎ, Îł, {Ď . Îş}i  Natasha Alechina  Reasoning about plan revision  TIME 2012  29 / 46  Transitions: PR rules  plan revision rules pj = Ďj â Î˛j | Ď 0 j  âi Ď 6|=cwa preci (Îą) Ď |=cwa Î˛j  hĎ, Îł, {Ďj = Îą; Ď . Îş}i ââ hĎ, Îł, {Ď 0 j . Îş}i  Ď 6|=cwa Î˛  Ď |=cwa Î˛j  hĎ, Îł, {Ďj = Î˛? ; Ď . Îş}i ââ hĎ, Îł, {Ď 0 j . Îş}i  Ď |=cwa Î˛j  hĎ, Îł, {Ďj = ÎąĚ; Ď . Îş}i ââ hĎ, Îł, {Ď 0 j ; Ď . Îş}i  where ÎąĚ is the name of an abstract plan.  Natasha Alechina  Reasoning about plan revision  TIME 2012  30 / 46  State of the art  State of the art in model-checking agent programs  Model-checking AgentSpeak (Promela, Spin)  Rafael H. Bordini, Michael Fisher, Carmen Pardavila, Michael  Wooldridge: Model checking AgentSpeak. AAMAS 2003:409-416  General platform for model-checking BDI agents (AIL and AJPF)  Louise A. Dennis, Michael Fisher, Matthew P. Webster, Rafael H.  Bordini: Model checking agent programming languages. Autom.  Softw. Eng. 19(1): 5-63 (2012)  Work with Goal, 3/2APL,...  Natasha Alechina  Reasoning about plan revision  TIME 2012  31 / 46  State of the art  Challenges  In common with general model-checking: scalability issues  In common with general (software) model-checking: hard to deal  with an infinite number of possible inputs/events, first-order  properties  I think there is still no system specification language at the right  level of abstraction  Beliefs, goals, plans, etc. are treated as just ordinary data  structures: same as lists of strings or some other âdumbâ values  However, they do have some logical structure (e.g. closure under  the agentâs reasoning rules) and connections to each other, which  should be used, in a transparent fashion (use something more like  Maude?)  The most interesting logical challenge here I think is the logic of  having committed to a set of intentions  Natasha Alechina  Reasoning about plan revision  TIME 2012  32 / 46  State of the art  What does having a set of intentions mean  If an agentâs set of intentions is {a; b; c, d; e; f } then it is easy to  figure out what the possible actions by the agent are (a and d); for  more general plans it is more complicated, but also well defined  no logic with explicit adopted plans (in the logical language), apart  from TCS11 (for single agent/single plan) and a paper in informal  proceedings of DALT 2009.  there are logics with explicit strategies (Simon and Ramanujam  2008,2009), but strategies and plans are not exactly the same and  logics have no âhe has adopted this strategyâ operator  Natasha Alechina  Reasoning about plan revision  TIME 2012  33 / 46  State of the art  Verification by theorem proving  State properties of the system as axioms (completely axiomatise  the operational semantics)  Prove that the desired property logically follows from them  This is a more complex problem than model-checking, but it is  easier to deal with first-order, infinite domains, etc.  Natasha Alechina  Reasoning about plan revision  TIME 2012  34 / 46  Logic  Signature of an agent program  The signature of an agent program R is defined as R = hP, PG,  ÂŻ Act, Plani  PR, Ac, Ac,  P is a set of belief and goal atoms  PG is a set of planning goal rules, ri = Îşi â Î˛i | Ďi  PR is a set of plan revision rules, pj = Ďj â Î˛j | Ďj0  Ac is a set of belief update actions occurring in the plans of PG and  PR rules  ÂŻ is a set of abstract plans occurring in the plans of PG and PR  Ac  rules  Act is the set of specifications for belief update actions Ac  Plan is the set of all possible Ď . Îş pairs where Îş is one of the  agentâs goals and Ď is a plan occurring in PG and PR rules or a  suffix of such a plan  Natasha Alechina  Reasoning about plan revision  TIME 2012  35 / 46  Logic  Language of PDL-3APL  program expressions:  ÂŻ | Î´r i | Î´p | Ď1 ; Ď2 | Ď1 âŞ Ď2 | Ďâ  Ď ::= Îą â Ac | t(Ď) | aĚ â Ac  j  formula:  Ď ::= Bp | Gp | G â p | x | P Îş Ď | P |ÂŹĎ | Ď1 â§ Ď2 | hĎiĎ  Natasha Alechina  Reasoning about plan revision  TIME 2012  36 / 46  Logic  Models of PDL-3APL  ÂŻ Act, Plani be the signature of an agent  Let R = hP, PG, PR, Ac, Ac,  program. A PDL-3APL model M relative to R is defined as  M = (W , V , RÎą , Rt(Ď) , RÎąĚ , RÎ´r i , RÎ´p j )  where  W is a non-empty set of states.  V = (Vb , Vg , Vc , Vp ) such that for every s â W :  Vb (s) = {p1 , . . . , pm : pi â P} is the set of the agentâs beliefs in s;  Vg (s) = {( â )u1 , . . . , ( â )un : ui â P} is the set of the agentâs goals  in s (note that Vg assigns literals rather than propositional  variables);  Vc (s) is either an empty set or {x};  Vp (s) is either the empty set or a singleton set {Ď . Îş}, where Ď is  the agentâs plan in s and Îş is the goal(s) achieved by this plan  RÎą , Rt(Ď) , RÎąĚ , RÎ´r i , RÎ´p i are binary relations on W  Natasha Alechina  Reasoning about plan revision  TIME 2012  37 / 46  Logic  Conditions on models  C1 Vg (s) âŠ Vb (s) = â and {p : â p â Vg (s)} â Vb (s)  C2 If Vp (s) = {Îą; Ď . Îş}, Vb (s) |=cwa preci (Îą) and x 6â Vc (s), then  there is an RÎą transition to a state s0 where Vb (s0 ) = Ti,j (Îą, Vb (s)),  Vg (s0 ) = Vg (s) \ ({p : p â Vb (s0 )} âŞ { â p : p 6â Vb (s0 )}) and if  Vb (s0 ) 6|=cwa Îş, Vp (s0 ) = {Ď . Îş}.  If Vb (s0 ) |=cwa Îş, x â Vc (s0 ) and Vp (s0 ) = {}.  C3âC10 similarly correspond to operational semantics in non-x  states  Natasha Alechina  Reasoning about plan revision  TIME 2012  38 / 46  Logic  Conditions for exceptional states  Condition for non-executable actions: if Vp (s) = {Îą; Ď . Îş},  Vb (s) 6|=cwa preci (Îą), and x 6â Vc (s), then there is an RÎą  transition to a state s0 where x â Vc (s0 ).  Condition for executing in exceptional states: if x â Vc (s) then  there are RÎą , RÎąĚ and Rt(Ď) transitions from state s to itself  Condition for PR rules: if x â Vc (s), Vp (s) = {Ďj . Îş},  Vb (s) |=cwa Î˛j , then there is a RÎ´p j transition to a state s0 where  Vp (s0 ) = {Ďj0 . Îş} and x 6â Vc (s0 ) (where pj = Ďj â Î˛j | Ďj0 ).  Natasha Alechina  Reasoning about plan revision  TIME 2012  39 / 46  Logic  Satisfaction  M, s |= Bp iff p â Vb (s)  M, s |= Gp iff p â Vg (s)  M, s |= G â p iff â p â Vg (s)  M, s |= x iff x â Vc (s)  M, s |= P Îş Ď iff Vp (s) = {Ď . Îş}  M, s |= P iff Vp (s) = {}  M, s |= ÂŹĎ iff M, s 6|= Ď  M, s |= Ď1 â§ Ď2 iff M, s |= Ď1 and M, s |= Ď2  M, s |= hĎiĎ iff there exists s0 such that RĎ (s, s0 ) and M, s0 |= Ď.  Natasha Alechina  Reasoning about plan revision  TIME 2012  40 / 46  Logic  Translation into PDL  fb : fb (p) = Bp; fb (Ď and Ď) = fb (Ď) â§ fb (Ď);  fb (Ď or Ď) = fb (Ď) â¨ fb (Ď)  fg (p) = Gp; fg ( â p) = G â p  fp :  fp (Îą) = Îą  fp (Ď?) = t(Ď)  fp (ÎąĚ) = ÎąĚ  fp (Ď1 ; Ď2 ) = fp (Ď1 ); fp (Ď2 )  fp (if Ď then Ď1 else Ď2 ) = t(Ď); fp (Ď1 )) âŞ (t(ÂŹĎ); fp (Ď2 ))  fp (while Ď do Ď) = (t(Ď); fp (Ď))â ; t(ÂŹĎ).  Natasha Alechina  Reasoning about plan revision  TIME 2012  41 / 46  Logic  Axioms  A1 Bp â ÂŹGp  A2 G â p â Bp  0  A3a P Îş Ď â ÂŹP Îş Ď 0 where Ď 0 6= Ď or Îş0 6= Îş  W  A3b P â¨ Ď.ÎşâPlan P Îş Ď  BA1 ÂŹx â§ P Îş (Îą; Ď) â§ fb (preci (Îą)) â§ Ď â§ Ď 0 â hÎąi(  (fb (postij (Îą))â§ÂŹfb (Îş)â§P Îş Ďâ§Ď)â¨(fb (postij (Îą))â§fb (Îş)â§x â§Pâ§Ď 0 ))  where Ď, Ď 0 are any formulas not containing plan expressions or  literals in fb (postij (Îą)), and in addition Ď 0 does not contain x  ÂŻ  BA2a ÂŹx â§ P Îş Ď â [u]âĽ where Ď 6= u; Ď 0 and u â Ac âŞ Ac  BA2b ÂŹx â§ P Îş Ď â [t(Ď)]âĽ if Ď does not start with a belief test action Ď?  or a conditional plan test on Ď where Ď = Ď or Ď = ÂŹĎ  Natasha Alechina  Reasoning about plan revision  TIME 2012  42 / 46  Logic  Axioms continued  V  V 0  Îş (Îą; Ď) â§ f (prec (Îą)) â§  BA3 ÂŹx  â§  P  Ď  â§  b  j  i  j  j Ďj â [Îą](  W  Îş  Wj ( fb (postij (Îą)) â§ ÂŹfb (Îş) â§ P Ď â§ Ďj )0 â¨  j ( fb (postij (Îą)) â§ fb (Îş) â§ x â§ P â§ Ďj ))  where Ďj and Ďj0 are any formulas not containing plan expressions  or literals in fb (postij (Îą)), and in addition Ďj0 does not contain x  BA4 ÂŹx â§ P Îş (Ď? ; Ď) â§ fb (Ď) â§ Ďnp â h[t(Ď)]i(P Îş Ď â§ Ďnp )  V  BA5 ÂŹx â§ P Îş (Îą; Ď) â§ i ÂŹfb (preci (Îą)) â§ Ďnx â h[Îą]i(x â§ Ďnx )  BA6 ÂŹx â§ P Îş (Ď? ; Ď) â§ ÂŹfb (Ď) â§ Ďnx â h[t(Ď)]i(x â§ Ďnx )  BA7 ÂŹx â§ P Îş (ÎąĚ; Ď) â§ Ďnx â h[ÎąĚ]i(x â§ Ďnx )  BA8 x â§ Ď â h[u]iĎ where u is Îą, t(Ď) or ÎąĚ  Natasha Alechina  Reasoning about plan revision  TIME 2012  43 / 46  Logic  Axioms continued  CP1 ÂŹx â§ P Îş (Ďif ; Ď) â§ fb (Ď) â§ Ďnp â h[t(Ď)]i(P Îş Ď1 ; Ď â§ Ďnp ), where Ďif is  of the form if Ď then Ď1 else Ď2  CP2 ÂŹx â§ P Îş (Ďif ; Ď) â§ ÂŹfb (Ď) â§ Ďnp â h[t(ÂŹĎ)]i(P Îş Ď2 ; Ď â§ Ďnp ), where  Ďif is as in CP1  CP3 ÂŹx â§ P Îş (Ďwh ; Ď) â§ fb (Ď) â§ Ďnp â h[t(Ď)]i(P Îş Ď1 ; Ďwh ; Ď â§ Ďnp ), where  Ďwh is of the form while Ď do Ď1  CP4 ÂŹx â§ P Îş (Ďwh ; Ď) â§ ÂŹfb (Ď) â§ Ďnp â h[t(ÂŹĎ)]i(P Îş Ď â§ Ďnp ), where Ďwh  is as in CP3  CP5 ÂŹx â§ (P Îş Ďif â¨ P Îş Ďwh ) â§ ÂŹfb (Ď) â [t(Ď)]âĽ where Ďif and Ďwh are as  above  PG1 P â§ fg (Îşi ) â§ fb (Î˛i ) â§ Ďnpx â h[Î´r i ]i(ÂŹx â§ P Îşi Ďi â§ Ďnpx )  PG2 ÂŹP â¨ ÂŹfg (Îşi ) â¨ ÂŹfb (Î˛i ) â [Î´r i ]âĽ  PR1 x â§ P Îş Ďj â§ fb (Î˛j ) â§ Ďnpx â h[Î´p j ]i(ÂŹx â§ P Îş Ďj0 â§ Ďnpx )  PR2 ÂŹx â¨ ÂŹP Îş Ďj â¨ ÂŹfb (Î˛j ) â [Î´p j ]âĽ  Natasha Alechina  Reasoning about plan revision  TIME 2012  44 / 46  Logic  Translation of the program  tr (R) = (âŞi (Î´r i ; fp (Ďi ))  S  âŞj (Î´p j ; fp (Ďj0 )))+  Theorem: tr (R) picks out exactly those paths in a model which  correspond to an execution of the program  Can verify liveness and safety properties by checking whether  htr (R)iĎ and [tr (R)]Ď are entailed by the formulas describing initial  conditions  complications: encoding plan expressions; encoding properties  which hold along a path (Fahad Khan 2012, Regular Path  Temporal Logic)  Natasha Alechina  Reasoning about plan revision  TIME 2012  45 / 46  Logic  Conclusions  agent programs can be verified just as ordinary programs  however they have additional properties which it may be possible  to expoit  one of the properties is having an explicit set of plans, which  seems to be an interesting logical property  may be also of interest for game logics (being able to say âthis  player is going to play this strategyâ rather than âif this player plays  this strategyâ)  Natasha Alechina  Reasoning about plan revision  TIME 2012  46 / 46 